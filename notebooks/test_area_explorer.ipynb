{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Authors: Justin Zarb\n",
    "###############\n",
    "\n",
    "import os\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import sys\n",
    "from urllib.parse import urlencode\n",
    "import pandas as pd\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "from src.streamlit_functions import get_nodes_with_tags_in_bbox, count_tag_frequency_in_nodes, gdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import openai\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import JSONLoader, CSVLoader\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACE_NAME = \"Alt-Treptow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all nodes in location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_query = f\"\"\"\n",
    "[out:json][timeout:25];\n",
    "area[name=\"{PLACE_NAME}\"]->.searchArea;\n",
    "(node(area.searchArea);\n",
    "way(area.searchArea);\n",
    "relation(area.searchArea););out body;>;out skel qt;\n",
    "\"\"\"\n",
    "response = requests.get(overpass_url, data=overpass_query)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Request succeeded, process the data here (response.json() or response.text)\n",
    "    data = response.json()\n",
    "else:\n",
    "    # Request failed, check the error message\n",
    "    error_message = response.text\n",
    "    print(f\"Error: {response.status_code} - {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4026, 2)\n",
      "       key            value\n",
      "0  highway  traffic_signals\n",
      "0  highway   turning_circle\n",
      "0  highway         crossing\n",
      "0  highway         bus_stop\n",
      "0  highway     construction\n"
     ]
    }
   ],
   "source": [
    "def tag_frequency_from_result(data):\n",
    "    nodes = [element for element in data[\"elements\"]]\n",
    "    tag_frequency = json.dumps(count_tag_frequency_in_nodes(nodes))\n",
    "    return tag_frequency\n",
    "\n",
    "tag_frequency = tag_frequency_from_result(data)\n",
    "d = json.loads(tag_frequency)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "tag_frequency_df = pd.DataFrame(list(d.items()), columns=['key', 'value'])\n",
    "\n",
    "# Write each value to its own row\n",
    "tag_frequency_df = tag_frequency_df.explode(\"value\")\n",
    "\n",
    "#Output option 3:\n",
    "dirty_file = f\"{PLACE_NAME}_node_tags.csv\"\n",
    "tag_frequency_df.to_csv(dirty_file, index=False, header=True)\n",
    "\n",
    "print(tag_frequency_df.shape)\n",
    "print(tag_frequency_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output option 1: Json file with full node data: {\"elements\":[]}\n",
    "tagged_nodes_filename = f\"{PLACE_NAME}_tagged_nodes.json\"\n",
    "\n",
    "if os.path.exists(tagged_nodes_filename):\n",
    "    tagged_nodes = [d for d in data[\"elements\"] if \"tags\" in d]\n",
    "    data_tagged_only = {\"elements\":tagged_nodes}\n",
    "\n",
    "    with open(tagged_nodes_filename, \"w\") as file:\n",
    "        # write to file\n",
    "        file.write(json.dumps(data_tagged_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output option 2: \n",
    "\n",
    "\n",
    "node_tags_filename = f\"{PLACE_NAME}_node_tags.json\"\n",
    "if os.path.exists(node_tags_filename):\n",
    "    with open(node_tags_filename, \"w\") as file:\n",
    "        # write to file\n",
    "        file.write(tag_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store of all tags in area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"Alt-Treptow_node_tags.csv\"\n",
    ")\n",
    "raw_data = loader.load()\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_data)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db\u001b[39m.\u001b[39;49msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mPLACE_NAME\u001b[39m}\u001b[39;00m\u001b[39m_db\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "db.save(f\"{PLACE_NAME}_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='key: vending\\nvalue: drinks', metadata={'row': 2104, 'source': 'Alt-Treptow_node_tags_dirty.csv'}),\n",
       " Document(page_content='key: vending\\nvalue: drinks', metadata={'row': 2104, 'source': 'Alt-Treptow_node_tags_dirty.csv'}),\n",
       " Document(page_content='key: vending\\nvalue: drinks', metadata={'row': 2104, 'source': 'Alt-Treptow_node_tags_dirty.csv'}),\n",
       " Document(page_content='key: vending\\nvalue: drinks', metadata={'row': 3160, 'source': 'Alt-Treptow_node_tags.csv'}),\n",
       " Document(page_content='key: emergency\\nvalue: drinking_water', metadata={'row': 1907, 'source': 'Alt-Treptow_node_tags_dirty.csv'})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a simple cosine search\n",
    "db.similarity_search(\"trinkbrünnen\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retriever\n",
    " A retriever gets the most relevant documents for an unstructured query. ([Langchain Retrievers]( https://python.langchain.com/docs/modules/data_connection/retrievers/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Provide five possible key:value pairs to search for falafel restaurants',\n",
       " 'answer': \" Possible key:value pairs to search for falafel restaurants include: \\n- name: Jimmy's Falafel \\n- cuisine: falafel \\n- type: restaurant \\n- food: falafel \\n- location: city \\n\",\n",
       " 'sources': 'Alt-Treptow_node_tags_dirty.csv'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Provide five possible key:value pairs to search for falafel restaurants\"\n",
    "index.query_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiquery Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide key:value pairs for Zierbrunnen?', '2. What are some key:value pairs for Badestellen?', '3. Could you give me key:value pairs for Strandbäder, Freibäder, Schwimmhallen, Bänke, Picknicktische, and Trinkbrunnen?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are some locations where individuals can access drinking water, engage in water activities, or swim?', '2. Can you suggest places where people have the opportunity to drink water, play in the water, or go swimming?', \"3. I'm looking for places that offer drinking water, water play areas, or swimming opportunities. Any recommendations?\", '4. Where can people go to find drinking water, areas to splash around, or places to swim?', '5. Are there any specific locations that provide access to drinking water, areas for water play, or swimming spots?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='key: sport\\nvalue: swimming', metadata={'row': 2687, 'source': 'Alt-Treptow_node_tags.csv'}),\n",
       " Document(page_content='key: drinking_water\\nvalue: yes', metadata={'row': 1300, 'source': 'Alt-Treptow_node_tags_dirty.csv'}),\n",
       " Document(page_content='key: drinking_water\\nvalue: yes', metadata={'row': 1898, 'source': 'Alt-Treptow_node_tags.csv'})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions seperated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "# Other inputs\n",
    "question = \"places where people can drink water, splash around or swim\"\n",
    "\n",
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.get_relevant_documents(\n",
    "    question\n",
    ")\n",
    "unique_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data with OSMNX or Overpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSMNX\n",
    "fuel_stations = ox.geometries_from_place(PLACE_NAME, {\"wikipedia\": \"de:Liste der Straßenbrunnen im Berliner Bezirk Treptow-Köpenick\"})\n",
    "fuel_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the number of unique values for each of the tags. Maybe useful\n",
    "num_unique_values = {k: len(v) for k, v in json.loads(tag_frequency).items()}\n",
    "num_unique_values = {\n",
    "    k: v\n",
    "    for k, v in sorted(\n",
    "        num_unique_values.items(), key=lambda item: item[1], reverse=True\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = [\"Mitte, Berlin\"]\n",
    "places_gdf = ox.geocode_to_gdf(places)\n",
    "# bbox = [S, W, N, E]\n",
    "bounding_boxes = places_gdf.loc[:, [\"bbox_south\", \"bbox_west\", \"bbox_north\", \"bbox_east\",]]\n",
    "places_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ping_pong = {'version': 0.6, 'generator': 'Overpass API 0.7.60.6 e2dc3e5b', 'osm3s': {'timestamp_osm_base': '2023-06-29T15:35:14Z', 'timestamp_areas_base': '2023-06-29T12:13:45Z', 'copyright': 'The data included in this document is from www.openstreetmap.org. The data is made available under ODbL.'}, 'elements': [{'type': 'node', 'id': 6835150496, 'lat': 52.5226885, 'lon': 13.3979877, 'tags': {'leisure': 'pitch', 'sport': 'table_tennis', 'wheelchair': 'yes'}}, {'type': 'node', 'id': 6835150497, 'lat': 52.5227083, 'lon': 13.3978939, 'tags': {'leisure': 'pitch', 'sport': 'table_tennis', 'wheelchair': 'yes'}}, {'type': 'node', 'id': 6835150598, 'lat': 52.5229822, 'lon': 13.3965893, 'tags': {'access': 'customers', 'leisure': 'pitch', 'sport': 'table_tennis'}}, {'type': 'node', 'id': 6835150599, 'lat': 52.5229863, 'lon': 13.3964894, 'tags': {'access': 'customers', 'leisure': 'pitch', 'sport': 'table_tennis'}}]}\n",
    "\n",
    "df = pd.DataFrame(ping_pong[\"elements\"])\n",
    "df[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "keys = []\n",
    "nodes = []\n",
    "for _, row in bounding_boxes.iterrows():\n",
    "    nodes.append(get_nodes_with_tags_in_bbox(list(row)))\n",
    "    unique_tags_dict = count_tag_frequency(nodes)\n",
    "    num_unique_values = {k:len(v) for k, v in unique_tags_dict.items()}\n",
    "    num_unique_values = {\n",
    "        k: v\n",
    "        for k, v in sorted(\n",
    "            num_unique_values.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "unique_tags_dict_sorted = [unique_tags_dict[k] for k in list(num_unique_values.keys())]\n",
    "\n",
    "unique_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dict(d, substring):\n",
    "    search_words = [s.strip() for s in substring.split(\",\")]\n",
    "    print(search_words)\n",
    "    matches = {}\n",
    "    for s in search_words:\n",
    "    # Add key value pairs if a substring appears in either key or value. Value is a list of strings. return only the matching string\n",
    "        for key, value in d.items():\n",
    "            if s in key:\n",
    "                matches[key] = value\n",
    "            else:\n",
    "                for v in value:\n",
    "                    if s in v:\n",
    "                        if key in matches:\n",
    "                            matches[key].append(v)\n",
    "                        else:\n",
    "                            matches[key] = [v]\n",
    "    return matches\n",
    "\n",
    "search_dict(unique_tags_dict, \"history, historical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict(zip(places_gdf[\"display_name\"], places_gdf[\"projected_area\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_gdf[[\"projected_area\", \"area_unit\"]] = places_gdf.apply(lambda row: gdf_data(row, places_gdf.crs), axis=1)\n",
    "places_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in bounding_boxes.iterrows():\n",
    "    nodes.append(get_nodes_with_tags_in_bbox(list(row)))\n",
    "    keys = list(count_tag_frequency(nodes).keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "from pyproj import CRS\n",
    "import geopandas as gpd\n",
    "def gdf_data(gdf):\n",
    "    \"\"\"Get the area of a polygon\n",
    "    This method is not directly callable by the LLM\"\"\"\n",
    "    places_dict = {}\n",
    "    for index, row in gdf.iterrows():\n",
    "        print(index)\n",
    "        utm_zone = utm.latlon_to_zone_number(gdf.loc[[index], \"lat\"].values[0], gdf.loc[[index], \"lon\"].values[0])\n",
    "        south = gdf.loc[[index], \"lat\"].values[0] < 0\n",
    "        crs = CRS.from_dict({\"proj\": \"utm\", \"zone\": utm_zone, \"south\": south})\n",
    "        epsg_code = crs.to_authority()[1]\n",
    "        unit = list({ai.unit_name for ai in crs.axis_info})[0]\n",
    "        gdf_projected = gdf.loc[[index],:].to_crs(epsg_code)\n",
    "        area = gdf_projected.area.values[0]\n",
    "        places_dict[row[\"display_name\"]] = {\"area\":area,\n",
    "                                            \"unit\":unit}\n",
    "\n",
    "    return places_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_data(places_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
